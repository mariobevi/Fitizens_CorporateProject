{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b9d71de-38dd-4844-9a03-b04eaab1ede7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\57312\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import openvino as ov\n",
    "from openvino.runtime.ie_api import CompiledModel\n",
    "\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\n",
    "    url='https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/main/notebooks/utils/notebook_utils.py',\n",
    "    filename='notebook_utils.py'\n",
    ")\n",
    "import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4282ef2-4f3e-49cb-8823-71b0ccc71b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A directory where the model will be downloaded.\n",
    "base_model_dir = \"model\"\n",
    "# The name of the model from Open Model Zoo.\n",
    "model_name = \"action-recognition-0001\"\n",
    "# Selected precision (FP32, FP16, FP16-INT8).\n",
    "precision = \"FP16\"\n",
    "model_path_decoder = (\n",
    "    f\"model/intel/{model_name}/{model_name}-decoder/{precision}/{model_name}-decoder.xml\"\n",
    ")\n",
    "model_path_encoder = (\n",
    "    f\"model/intel/{model_name}/{model_name}-encoder/{precision}/{model_name}-encoder.xml\"\n",
    ")\n",
    "encoder_url = f\"https://storage.openvinotoolkit.org/repositories/open_model_zoo/temp/{model_name}/{model_name}-encoder/{precision}/{model_name}-encoder.xml\"\n",
    "decoder_url = f\"https://storage.openvinotoolkit.org/repositories/open_model_zoo/temp/{model_name}/{model_name}-decoder/{precision}/{model_name}-decoder.xml\"\n",
    "\n",
    "if not os.path.exists(model_path_decoder):\n",
    "    utils.download_ir_model(decoder_url, Path(model_path_decoder).parent)\n",
    "if not os.path.exists(model_path_encoder):\n",
    "    utils.download_ir_model(encoder_url, Path(model_path_encoder).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243703fd-a3ce-4375-939c-6572acdc6753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data\\kinetics.txt' already exists.\n",
      "['abseiling', 'air drumming', 'answering questions', 'applauding', 'applying cream', 'archery', 'arm wrestling', 'arranging flowers', 'assembling computer'] (400,)\n"
     ]
    }
   ],
   "source": [
    "# Download the text from the openvino_notebooks storage\n",
    "vocab_file_path = utils.download_file(\n",
    "    \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/text/kinetics.txt\",\n",
    "    directory=\"data\"\n",
    ")\n",
    "\n",
    "with vocab_file_path.open(mode='r') as f:\n",
    "    labels = [line.strip() for line in f]\n",
    "\n",
    "print(labels[0:9], np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4432139a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pull ups'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specified_list = ['clean and jerk','throwing ball','swinging legs','stretching leg','squat','situp','side kick',\n",
    "                  'push up','pull ups','snatch weight lifting','lunge','exercising with an exercise ball',\n",
    "                  'exercising arm','deadlifting','yoga','stretching arm']\n",
    "labels = ['no exercise' if  all(spec not in label.lower() for spec in specified_list) else label for label in labels]\n",
    "labels[255] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ebac336-20a5-40a5-8720-0ac371e90feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5386f695bff40cca822d40d7fd8e69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d84dc81a-73ed-44f8-a0e4-2c5617a1496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenVINO Runtime.\n",
    "core = ov.Core()\n",
    "\n",
    "\n",
    "def model_init(model_path: str, device: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Read the network and weights from a file, load the\n",
    "    model on CPU and get input and output names of nodes\n",
    "\n",
    "    :param:\n",
    "            model: model architecture path *.xml\n",
    "            device: inference device\n",
    "    :retuns:\n",
    "            compiled_model: Compiled model\n",
    "            input_key: Input node for model\n",
    "            output_key: Output node for model\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the network and corresponding weights from a file.\n",
    "    model = core.read_model(model=model_path)\n",
    "    # Compile the model for specified device.\n",
    "    compiled_model = core.compile_model(model=model, device_name=device)\n",
    "    # Get input and output names of nodes.\n",
    "    input_keys = compiled_model.input(0)\n",
    "    output_keys = compiled_model.output(0)\n",
    "    return input_keys, output_keys, compiled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e005cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Angles of Joints\n",
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec2ef408",
   "metadata": {},
   "outputs": [],
   "source": [
    "joints_dictionary = {'push up': [['LEFT_SHOULDER','LEFT_ELBOW','LEFT_WRIST'], ['RIGHT_SHOULDER','RIGHT_ELBOW','RIGHT_WRIST'],[155, 90]],\n",
    "                     'pull ups': [['LEFT_SHOULDER','LEFT_ELBOW','LEFT_WRIST'], ['RIGHT_SHOULDER','RIGHT_ELBOW','RIGHT_WRIST'],[155, 80]],\n",
    "                     'situp': [['LEFT_SHOULDER','LEFT_HIP','LEFT_ANKLE'], ['RIGHT_SHOULDER','RIGHT_HIP','RIGHT_ANKLE'],[160, 130]],\n",
    "                     'squat': [['LEFT_SHOULDER','LEFT_HIP','LEFT_KNEE'], ['RIGHT_SHOULDER','RIGHT_HIP','RIGHT_KNEE'],[155, 110]],\n",
    "                     'snatch weight lifting': [['LEFT_WRIST','LEFT_SHOULDER','LEFT_ANKLE'], ['RIGHT_WRIST','RIGHT_SHOULDER', 'RIGHT_ANKLE'],[155, 60]],\n",
    "                     'burpee': [['LEFT_HIP','LEFT_KNEE', 'LEFT_ANKLE'], ['RIGHT_HIP','RIGHT_KNEE', 'RIGHT_ANKLE'],[155, 90]]\n",
    "                     }\n",
    "#Returns the 3 angles that constitute a side of the body\n",
    "def initialize_joints(side_list, landmarks):\n",
    "    joint_coordinates = []\n",
    "\n",
    "    for joint_name in side_list:\n",
    "    # Get the index of the joint name in mp_pose.PoseLandmark enum\n",
    "        joint_index = getattr(mp_pose.PoseLandmark, joint_name).value\n",
    "        # Extract the x and y coordinates of the joint and append them to joint_coordinates\n",
    "        joint_x = landmarks[joint_index].x\n",
    "        joint_y = landmarks[joint_index].y\n",
    "        joint_coordinates.append([joint_x, joint_y])\n",
    "\n",
    "    return joint_coordinates[0], joint_coordinates[1], joint_coordinates[2]\n",
    "\n",
    "# Manual finder of exercise\n",
    "def print_actual_exercise(exercie_results_df):\n",
    "    label_probabilities = dict(zip(exercie_results_df['label'], exercie_results_df['probability']))\n",
    "\n",
    "    situp = label_probabilities.get('situp', None)\n",
    "    exercising_with_an_exercise_ball = label_probabilities.get('exercising with an exercise ball', None)\n",
    "    throwing_ball = label_probabilities.get('throwing ball', None)\n",
    "    stretching_leg = label_probabilities.get('stretching leg', None)\n",
    "\n",
    "    squat = label_probabilities.get('squat', None)\n",
    "    lunge = label_probabilities.get('lunge', None)\n",
    "    snatch_weight_lifting = label_probabilities.get('snatch weight lifting', None)\n",
    "    clean_and_jerk = label_probabilities.get('clean and jerk', None)\n",
    "\n",
    "    deadlifting = label_probabilities.get('deadlifting', None)\n",
    "    push_up = label_probabilities.get('push up', None)\n",
    "    exercising_arm = label_probabilities.get('exercising arm', None)\n",
    "    swinging_legs = label_probabilities.get('swinging legs', None)\n",
    "\n",
    "    stretching_arm = label_probabilities.get('stretching arm', None)\n",
    "    side_kick = label_probabilities.get('side kick', None)\n",
    "    pull_ups = label_probabilities.get('pull ups', None)\n",
    "    yoga = label_probabilities.get('yoga', None)\n",
    "\n",
    "    current_exercise = 'no exercise'\n",
    "    base_situp = situp + exercising_with_an_exercise_ball + throwing_ball + stretching_leg + yoga\n",
    "    base_squad = squat + lunge + snatch_weight_lifting + situp\n",
    "    base_snatch = snatch_weight_lifting + lunge + clean_and_jerk + deadlifting + squat\n",
    "    base_pushup = push_up + exercising_arm + stretching_leg + swinging_legs + stretching_arm + side_kick + exercising_with_an_exercise_ball\n",
    "    base_burpee = push_up + squat + exercising_arm + situp + lunge + throwing_ball\n",
    "\n",
    "    if pull_ups >= 0.8:\n",
    "        current_exercise = 'pull ups'\n",
    "    #    print(f'Recognized exercie: {current_exercise} with {pull_ups}')\n",
    "    elif situp >= 0.22:\n",
    "        if base_situp >= 0.6:\n",
    "            current_exercise = 'situp'\n",
    "    #        print(f'Recognized exercie: {current_exercise} with {base_situp}')\n",
    "    elif squat >= 0.4:\n",
    "        if base_squad >= 0.7 and snatch_weight_lifting < 0.1:\n",
    "            current_exercise = 'squat'\n",
    "    #        print(f'Recognized exercie: {current_exercise} with {base_squad}')\n",
    "    elif snatch_weight_lifting + clean_and_jerk + deadlifting > 0.15:\n",
    "        if  base_snatch > 0.5:\n",
    "            current_exercise = 'snatch weight lifting'\n",
    "    #        print(f'Recognized exercie: {current_exercise} with {base_snatch}')\n",
    "    elif push_up > 0.35 or stretching_leg > 0.6:\n",
    "        if base_pushup > 0.7:\n",
    "            current_exercise = 'push up'\n",
    "    #        print(f'Recognized exercie: {current_exercise} with {base_pushup}')\n",
    "    elif push_up < 0.25 and squat < 0.25 and situp < 0.25 and  base_burpee > 0.6:\n",
    "        current_exercise = 'burpee'\n",
    "        print(f'Recognized exercie: {current_exercise} with {base_pushup}')\n",
    "    #print(f'Recognized exercie: {current_exercise}')\n",
    "    return current_exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31aaf5a1-fd21-4d82-bed1-ea877e92d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder initialization\n",
    "input_key_en, output_keys_en, compiled_model_en = model_init(model_path_encoder, device.value)\n",
    "# Decoder initialization\n",
    "input_key_de, output_keys_de, compiled_model_de = model_init(model_path_decoder, device.value)\n",
    "\n",
    "# Get input size - Encoder.\n",
    "height_en, width_en = list(input_key_en.shape)[2:]\n",
    "# Get input size - Decoder.\n",
    "frames2decode = list(input_key_de.shape)[0:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "734df139-c3de-48c9-9f13-ec2415baa8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(frame: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Center crop squared the original frame to standardize the input image to the encoder model\n",
    "\n",
    "    :param frame: input frame\n",
    "    :returns: center-crop-squared frame\n",
    "    \"\"\"    \n",
    "    img_h, img_w, _ = frame.shape\n",
    "    min_dim = min(img_h, img_w)\n",
    "    start_x = int((img_w - min_dim) / 2.0)\n",
    "    start_y = int((img_h - min_dim) / 2.0)\n",
    "    roi = [start_y, (start_y + min_dim), start_x, (start_x + min_dim)]\n",
    "    return frame[start_y : (start_y + min_dim), start_x : (start_x + min_dim), ...], roi\n",
    "\n",
    "\n",
    "#Will crop and center the image on the identified body.\n",
    "def center_body (frame, thres = 0.05):\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "    landmarks = [[lm.x, lm.y] for lm in landmarks]\n",
    "    landmarks = np.array(landmarks)\n",
    "\n",
    "    #Get Landmark X and Y\n",
    "    x_coords = landmarks[:, 0]\n",
    "    y_coords = landmarks[:, 1]\n",
    "\n",
    "    # Find min and max x and y coordinates\n",
    "    min_x = round(max(np.amin(x_coords) - thres, 0) * frame.shape[1])\n",
    "    min_y = round(max(np.amin(y_coords) - thres, 0) * frame.shape[0])\n",
    "    max_x = round(min(np.amax(x_coords) + thres, 1) * frame.shape[1])\n",
    "    max_y = round(min(np.amax(y_coords) + thres, 1) * frame.shape[0])\n",
    "\n",
    "    # Create width and height.\n",
    "    w = max_x - min_x\n",
    "    h = max_y - min_y\n",
    "\n",
    "    # Determine size of the square frame\n",
    "    size = max(w, h)\n",
    "\n",
    "    # Calculate center of the bounding box\n",
    "    center_x = min_x + w // 2\n",
    "    center_y = min_y + h // 2\n",
    "\n",
    "    # Calculate coordinates for cropping\n",
    "    start_x = max(0, center_x - size // 2)\n",
    "    start_y = max(0, center_y - size // 2)\n",
    "    end_x = min(frame.shape[1], start_x + size)\n",
    "    end_y = min(frame.shape[0], start_y + size)\n",
    "\n",
    "    roi = [start_y, (start_y + end_y), start_x, (start_x + end_x)]\n",
    "    cropped_image = frame[start_y:end_y, start_x:end_x]\n",
    "    return cropped_image, roi\n",
    "\n",
    "\n",
    "def adaptive_resize(frame: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "     The frame going to be resized to have a height of size or a width of size\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param size: input size to encoder model\n",
    "    :returns: resized frame, np.array type\n",
    "    \"\"\"\n",
    "    h, w, _ = frame.shape\n",
    "    scale = size / min(h, w)\n",
    "    w_scaled, h_scaled = int(w * scale), int(h * scale)\n",
    "    if w_scaled == w and h_scaled == h:\n",
    "        return frame\n",
    "    #return cv2.resize(frame, (w_scaled, h_scaled))\n",
    "    return cv2.resize(frame, (size, size))\n",
    "\n",
    "\n",
    "def decode_output(probs: np.ndarray, labels: np.ndarray, top_k: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Decodes top probabilities into corresponding label names\n",
    "\n",
    "    :param probs: confidence vector for 400 actions\n",
    "    :param labels: list of actions\n",
    "    :param top_k: The k most probable positions in the list of labels\n",
    "    :returns: decoded_labels: The k most probable actions from the labels list\n",
    "              decoded_top_probs: confidence for the k most probable actions\n",
    "\n",
    "    top_ind = np.argsort(-1 * probs)[:top_k]\n",
    "    out_label = np.array(labels)[top_ind.astype(int)]\n",
    "    decoded_labels = [out_label[0][0], out_label[0][1], out_label[0][2]]\n",
    "    top_probs = np.array(probs)[0][top_ind.astype(int)]\n",
    "    decoded_top_probs = [top_probs[0][0], top_probs[0][1], top_probs[0][2]]\n",
    "\n",
    "    \"\"\"\n",
    "    # Step 1: Create a DataFrame with columns 'label' and 'probability'\n",
    "    df = pd.DataFrame({'label': labels, 'probability': probs[0]})\n",
    "    # Step 3: Group by 'label' with the sum of probabilities\n",
    "    grouped_df = df.groupby('label')['probability'].sum().reset_index()\n",
    "    current_exercise = print_actual_exercise(grouped_df)\n",
    "\n",
    "    # Step 4: Get the top k results\n",
    "    sorted_df = grouped_df.sort_values(by='probability', ascending=False).head(top_k)\n",
    "    \n",
    "    # Get decoded labels and probabilities\n",
    "    decoded_labels = sorted_df['label'].tolist()\n",
    "    decoded_top_probs = sorted_df['probability'].tolist()\n",
    "\n",
    "    return decoded_labels, decoded_top_probs, current_exercise\n",
    "\n",
    "\n",
    "def rec_frame_display(frame: np.ndarray, roi) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw a rec frame over actual frame\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param roi: Region of interest, image section processed by the Encoder\n",
    "    :returns: frame with drawed shape\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    cv2.line(frame, (roi[2] + 3, roi[0] + 3), (roi[2] + 3, roi[0] + 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[2] + 3, roi[0] + 3), (roi[2] + 100, roi[0] + 3), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[1] - 3), (roi[3] - 3, roi[1] - 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[1] - 3), (roi[3] - 100, roi[1] - 3), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[0] + 3), (roi[3] - 3, roi[0] + 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[0] + 3), (roi[3] - 100, roi[0] + 3), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[2] + 3, roi[1] - 3), (roi[2] + 3, roi[1] - 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[2] + 3, roi[1] - 3), (roi[2] + 100, roi[1] - 3), (0, 200, 0), 2)\n",
    "    # Write ROI over actual frame\n",
    "    FONT_STYLE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    org = (roi[2] + 3, roi[1] - 3)\n",
    "    org2 = (roi[2] + 2, roi[1] - 2)\n",
    "    FONT_SIZE = 0.5\n",
    "    FONT_COLOR = (0, 200, 0)\n",
    "    FONT_COLOR2 = (0, 0, 0)\n",
    "    cv2.putText(frame, \"ROI\", org2, FONT_STYLE, FONT_SIZE, FONT_COLOR2)\n",
    "    cv2.putText(frame, \"ROI\", org, FONT_STYLE, FONT_SIZE, FONT_COLOR)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def display_text_fnc(frame: np.ndarray, display_text: str, index: int):\n",
    "    \"\"\"\n",
    "    Include a text on the analyzed frame\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param display_text: text to add on the frame\n",
    "    :param index: index line dor adding text\n",
    "\n",
    "    \"\"\"\n",
    "    # Configuration for displaying images with text.\n",
    "    FONT_COLOR = (0, 255, 0)\n",
    "    FONT_COLOR2 = (0, 0, 0)\n",
    "    FONT_STYLE = cv2.FONT_HERSHEY_DUPLEX\n",
    "    FONT_SIZE = 1\n",
    "    TEXT_VERTICAL_INTERVAL = 25\n",
    "    TEXT_LEFT_MARGIN = 15\n",
    "    # ROI over actual frame\n",
    "    #(processed, roi) = center_crop(frame)\n",
    "    # Draw a ROI over actual frame.\n",
    "    #frame = rec_frame_display(frame, roi)\n",
    "    # Put a text over actual frame.\n",
    "    text_loc = (TEXT_LEFT_MARGIN, TEXT_VERTICAL_INTERVAL * (index + 1))\n",
    "    text_loc2 = (TEXT_LEFT_MARGIN + 1, TEXT_VERTICAL_INTERVAL * (index + 1) + 1)\n",
    "    cv2.putText(frame, display_text, text_loc2, FONT_STYLE, FONT_SIZE, FONT_COLOR2, 2)\n",
    "    cv2.putText(frame, display_text, text_loc, FONT_STYLE, FONT_SIZE, FONT_COLOR, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67522f94-596e-4c77-8e3e-c6e252727b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(frame: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preparing frame before Encoder.\n",
    "    The image should be scaled to its shortest dimension at \"size\"\n",
    "    and cropped, centered, and squared so that both width and\n",
    "    height have lengths \"size\". The frame must be transposed from\n",
    "    Height-Width-Channels (HWC) to Channels-Height-Width (CHW).\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param size: input size to encoder model\n",
    "    :returns: resized and cropped frame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        (preprocessed, roi) = center_body(frame)\n",
    "        preprocessed = adaptive_resize(preprocessed, size)\n",
    "    #print('prepro_body: ', preprocessed.shape)\n",
    "    except:\n",
    "        # Adaptative resize\n",
    "        preprocessed = adaptive_resize(frame, size)\n",
    "        # Center_crop\n",
    "        (preprocessed, roi) = center_crop(preprocessed)\n",
    "        \n",
    "    # Transpose frame HWC -> CHW\n",
    "    preprocessed = preprocessed.transpose((2, 0, 1))[None,]  # HWC -> CHW\n",
    "    return preprocessed, roi\n",
    "\n",
    "\n",
    "def encoder(\n",
    "    preprocessed: np.ndarray,\n",
    "    compiled_model: CompiledModel\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    Encoder Inference per frame. This function calls the network previously\n",
    "    configured for the encoder model (compiled_model), extracts the data\n",
    "    from the output node, and appends it in an array to be used by the decoder.\n",
    "\n",
    "    :param: preprocessed: preprocessing frame\n",
    "    :param: compiled_model: Encoder model network\n",
    "    :returns: encoder_output: embedding layer that is appended with each arriving frame\n",
    "    \"\"\"\n",
    "    output_key_en = compiled_model.output(0)\n",
    "\n",
    "    # Get results on action-recognition-0001-encoder model\n",
    "    infer_result_encoder = compiled_model([preprocessed])[output_key_en]\n",
    "    return infer_result_encoder\n",
    "\n",
    "\n",
    "def decoder(encoder_output: List, compiled_model_de: CompiledModel) -> List:\n",
    "    \"\"\"\n",
    "    Decoder inference per set of frames. This function concatenates the embedding layer\n",
    "    froms the encoder output, transpose the array to match with the decoder input size.\n",
    "    Calls the network previously configured for the decoder model (compiled_model_de), extracts\n",
    "    the logits and normalize those to get confidence values along specified axis.\n",
    "    Decodes top probabilities into corresponding label names\n",
    "\n",
    "    :param: encoder_output: embedding layer for 16 frames\n",
    "    :param: compiled_model_de: Decoder model network\n",
    "    :returns: decoded_labels: The k most probable actions from the labels list\n",
    "              decoded_top_probs: confidence for the k most probable actions\n",
    "    \"\"\"\n",
    "    # Concatenate sample_duration frames in just one array\n",
    "    decoder_input = np.concatenate(encoder_output, axis=0)\n",
    "    # Organize input shape vector to the Decoder (shape: [1x16x512]]\n",
    "    decoder_input = decoder_input.transpose((2, 0, 1, 3))\n",
    "    decoder_input = np.squeeze(decoder_input, axis=3)\n",
    "    output_key_de = compiled_model_de.output(0)\n",
    "    # Get results on action-recognition-0001-decoder model\n",
    "    result_de = compiled_model_de([decoder_input])[output_key_de]\n",
    "    # Normalize logits to get confidence values along specified axis\n",
    "    probs = softmax(result_de - np.max(result_de))\n",
    "    df = pd.DataFrame({'label': labels, 'probability': probs[0]})\n",
    "    grouped_df = df.groupby('label')['probability'].sum().reset_index()\n",
    "    sorted_df = grouped_df.sort_values(by='probability', ascending=False)\n",
    "    #print(sorted_df.head(10))\n",
    "    # Decodes top probabilities into corresponding label names\n",
    "    decoded_labels, decoded_top_probs, current_exercise = decode_output(probs, labels, top_k=3)\n",
    "    return decoded_labels, decoded_top_probs, current_exercise\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes logits to get confidence values along specified axis\n",
    "    x: np.array, axis=None\n",
    "    \"\"\"\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9dde48b-ecbe-4691-9616-a289a93a4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_action_recognition(\n",
    "    source: str = \"0\",\n",
    "    flip: bool = True,\n",
    "    use_popup: bool = False,\n",
    "    compiled_model_en: CompiledModel = compiled_model_en,\n",
    "    compiled_model_de: CompiledModel = compiled_model_de,\n",
    "    skip_first_frames: int = 0,\n",
    "):\n",
    "    size = height_en  # Encoder requiered size\n",
    "    sample_duration = frames2decode  # Number of frames that decoder needs\n",
    "    # Select FPS source.\n",
    "    fps = 20\n",
    "    player = None\n",
    "    exercise_dict = {} #Store repetitions per exercise on the video\n",
    "    record_video = {'no exercise':[]}\n",
    "\n",
    "    #Mediapipe Pose Detection\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    \n",
    "        try:\n",
    "            # Create a video player.\n",
    "            player = utils.VideoPlayer(source, flip=flip, fps=fps, skip_first_frames=skip_first_frames)\n",
    "            # Start capturing.\n",
    "            player.start()\n",
    "            if use_popup:\n",
    "                title = \"Press ESC to Exit\"\n",
    "                cv2.namedWindow(title, cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "            processing_times = collections.deque()\n",
    "            processing_time = 0\n",
    "            encoder_output = []\n",
    "            decoded_labels = ['no exercise', 0, 0]\n",
    "            actual_exercise = 'no exercise'\n",
    "            decoded_top_probs = [0, 0, 0]\n",
    "            counter = 0\n",
    "            stage = None    #Stage inside a Cycle. Extension or Contraction\n",
    "\n",
    "\n",
    "            # Create a text template to show inference results over video.\n",
    "            text_inference_template = \"Infer Time:{Time:.1f}ms,{fps:.1f}FPS\"\n",
    "            text_template = \"{label},{conf:.2f}%\"\n",
    "\n",
    "            while True:\n",
    "                counter = counter + 1\n",
    "\n",
    "                # Read a frame from the video stream.\n",
    "                frame = player.next()\n",
    "                if frame is None:\n",
    "                    print(\"Source ended\")\n",
    "                    break\n",
    "\n",
    "                scale = 1280 / max(frame.shape)\n",
    "\n",
    "\n",
    "                ####### Define Current Exercise Probabilities\n",
    "                if counter % 2 == 0:\n",
    "                    # Preprocess frame before Encoder.\n",
    "                    (preprocessed, _) = preprocessing(frame, size)\n",
    "                    #record_video['no exercise'].append(preprocessed)\n",
    "\n",
    "                    # Measure processing time.\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    # Encoder Inference per frame\n",
    "                    encoder_output.append(encoder(preprocessed, compiled_model_en))\n",
    "\n",
    "                    # Decoder inference per set of frames\n",
    "                    # Wait for sample duration to work with decoder model.\n",
    "                    if len(encoder_output) == sample_duration:\n",
    "                        decoded_labels, decoded_top_probs, actual_exercise = decoder(encoder_output, compiled_model_de)\n",
    "                        encoder_output = []\n",
    "\n",
    "                        if actual_exercise != 'no exercise':\n",
    "                            print(actual_exercise)\n",
    "\n",
    "                    # Inference has finished. Display the results.\n",
    "                    stop_time = time.time()\n",
    "\n",
    "                    # Calculate processing time.\n",
    "                    processing_times.append(stop_time - start_time)\n",
    "\n",
    "                    # Use processing times from last 200 frames.\n",
    "                    if len(processing_times) > 200:\n",
    "                        processing_times.popleft()\n",
    "\n",
    "                    # Mean processing time [ms]\n",
    "                    processing_time = np.mean(processing_times) * 1000\n",
    "                    fps = 1000 / processing_time\n",
    "\n",
    "                ####### Define exercise repetitions \n",
    "                if actual_exercise == 'no exercise':\n",
    "                        count_ext = 0\n",
    "                        count_con = 0\n",
    "\n",
    "                elif actual_exercise != 'no exercise':\n",
    "                    # Recolor image to RGB\n",
    "                    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    image.flags.writeable = False\n",
    "                \n",
    "                    # Make detection\n",
    "                    results = pose.process(image)\n",
    "                \n",
    "                    # Recolor back to BGR\n",
    "                    image.flags.writeable = True\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                    try:\n",
    "                        landmarks = results.pose_landmarks.landmark\n",
    "                        \n",
    "                        # Get coordinates\n",
    "                        A,B,C = initialize_joints(joints_dictionary[actual_exercise][0],landmarks)\n",
    "                        D,E,F = initialize_joints(joints_dictionary[actual_exercise][1],landmarks)\n",
    "                        \n",
    "                        # Calculate angle\n",
    "                        angle_l = calculate_angle(A, B, C)\n",
    "                        angle_r = calculate_angle(D, E, F)\n",
    "                        #print(f'angle izq: {angle_l}  -  angle der: {angle_r}')\n",
    "\n",
    "                        AD = [(A[0] + D[0]) / 2, (A[1] + D[1]) / 2]\n",
    "                        BE = [(B[0] + E[0]) / 2, (B[1] + E[1]) / 2]\n",
    "                        CF = [(C[0] + F[0]) / 2, (C[1] + F[1]) / 2]\n",
    "                        angle_mid = calculate_angle(AD, BE, CF)\n",
    "                        #print(f'angle izq: {angle_l}  -  angle der: {angle_r}  -  angle_mid: {angle_mid}')\n",
    "\n",
    "                        \n",
    "                        #Special Case for Snatch (One Arm snatch)\n",
    "                        if actual_exercise == 'snatch weight lifting':\n",
    "                        \n",
    "                            #Extension\n",
    "                            if angle_mid > joints_dictionary[actual_exercise][2][0] and stage != \"extension\":\n",
    "                                stage = \"extension\"\n",
    "                                count_ext = 1\n",
    "                                #print(f'stage: {stage}  -  angle mid: {angle_mid}')\n",
    "                            \n",
    "                            #Contraction\n",
    "                            if angle_mid < joints_dictionary[actual_exercise][2][1] and stage !='contraction':\n",
    "                                stage=\"contraction\"\n",
    "                                count_con = 1\n",
    "                                #print(f'stage: {stage}  -  angle mid: {angle_mid}')\n",
    "\n",
    "                        #Special Case for Situp (MiddlePoint)\n",
    "                        elif actual_exercise == 'situp':\n",
    "                        \n",
    "                            #Extension\n",
    "                            if angle_mid > joints_dictionary[actual_exercise][2][0] and stage != \"extension\":\n",
    "                                stage = \"extension\"\n",
    "                                count_ext = 1\n",
    "                                #print(f'stage: {stage}  -  angle mid: {angle_mid}')\n",
    "                            \n",
    "                            #Contraction\n",
    "                            if angle_mid < joints_dictionary[actual_exercise][2][1] and stage !='contraction':\n",
    "                                stage=\"contraction\"\n",
    "                                count_con = 1\n",
    "                                #print(f'stage: {stage}  -  angle mid: {angle_mid}')\n",
    "                        \n",
    "                        else:\n",
    "\n",
    "                            #Extension\n",
    "                            if angle_l > joints_dictionary[actual_exercise][2][0] and angle_r > joints_dictionary[actual_exercise][2][0] and stage != \"extension\":\n",
    "                                stage = \"extension\"\n",
    "                                count_ext = 1\n",
    "                                #print(f'stage: {stage}  -  angulo izq: {angle_l}  -  angulo der: {angle_r}')\n",
    "                            \n",
    "                            #Contraction\n",
    "                            if angle_l < joints_dictionary[actual_exercise][2][1] and angle_r < joints_dictionary[actual_exercise][2][1] and stage !='contraction':\n",
    "                                stage=\"contraction\"\n",
    "                                count_con = 1\n",
    "                                #print(f'stage: {stage}  -  angulo izq: {angle_l}  -  angulo der: {angle_r}')\n",
    "\n",
    "                        #Complete Cycle, Add 1 To counter\n",
    "                        if count_ext + count_con == 2:\n",
    "                            count_ext = 0 \n",
    "                            count_con = 0\n",
    "                            if actual_exercise not in exercise_dict:\n",
    "                                exercise_dict[actual_exercise] = 1\n",
    "                            else:\n",
    "                                exercise_dict[actual_exercise] += 1\n",
    "                            print(exercise_dict)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                # Adaptative resize for visualization.\n",
    "                if scale < 1:\n",
    "                    frame = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "                '''\n",
    "                # Visualize the results of the 400 activities\n",
    "                for i in range(0, 3):\n",
    "                    display_text = text_template.format(\n",
    "                        label=decoded_labels[i],\n",
    "                        conf=decoded_top_probs[i] * 100,\n",
    "                    )\n",
    "                    display_text_fnc(frame, display_text, i)\n",
    "                \n",
    "                display_text = text_inference_template.format(Time=processing_time, fps=fps)\n",
    "                display_text_fnc(frame, display_text, 3)\n",
    "\n",
    "                '''\n",
    "\n",
    "                # Results for the customized results\n",
    "                display_text_fnc(frame, f'Current Exercise: {actual_exercise}', 0)\n",
    "                display_text_fnc(frame, 'REPETITIONS:', 1)\n",
    "                \n",
    "                print_count = 0\n",
    "                for exer , reps in exercise_dict.items():\n",
    "                    display_text_fnc(frame, f'{exer} : {reps}', print_count + 2)\n",
    "                    print_count += 1\n",
    "                \n",
    "                record_video['no exercise'].append(frame)\n",
    "                \n",
    "\n",
    "                if use_popup:\n",
    "                    cv2.imshow(title, frame)\n",
    "                    key = cv2.waitKey(1)\n",
    "                    # escape = 27\n",
    "                    if key == 27:\n",
    "                        break\n",
    "                else:\n",
    "                    _, encoded_img = cv2.imencode(\".jpg\", frame, params=[cv2.IMWRITE_JPEG_QUALITY, 90])\n",
    "                    i = display.Image(data=encoded_img)\n",
    "                    display.clear_output(wait=True)\n",
    "                    display.display(i)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            if player is not None:\n",
    "                # Stop capturing.\n",
    "                player.stop()\n",
    "            if use_popup:\n",
    "                cv2.destroyAllWindows()\n",
    "    return exercise_dict, record_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15671edc-cc6e-4fc2-a83a-f867f4e585b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WEBCAM = True\n",
    "\n",
    "cam_id = 0\n",
    "#video_file = \"video_data/pull_up/pull_up_1.mp4\"\n",
    "video_file = \"video_data/sit_up/sit_up_2.mp4\"\n",
    "\n",
    "source = cam_id if USE_WEBCAM else video_file\n",
    "additional_options = {\"skip_first_frames\": 0, \"flip\": False} if not USE_WEBCAM else {\"flip\": True}\n",
    "exercise_dict, record = run_action_recognition(source=source, use_popup=True, **additional_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93c63799",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exercie_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mexercie_results\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Replace space with underscore (_) in label to create valid variable name\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     label_name \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Create variables dynamically using the modified label as the variable name\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exercie_results' is not defined"
     ]
    }
   ],
   "source": [
    "for index, row in exercie_results.iterrows():\n",
    "    # Replace space with underscore (_) in label to create valid variable name\n",
    "    label_name = row['label'].replace(' ', '_')\n",
    "    # Create variables dynamically using the modified label as the variable name\n",
    "    vars()[label_name] = row['probability']\n",
    "    print(label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7843a0f",
   "metadata": {},
   "source": [
    "## Record video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe1ef158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record['no exercise'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca437d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video created successfully: detect/try_1.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "fps = 16  # Adjust the frame rate as needed\n",
    "output_video_path = \"detect/try_2.mp4\"\n",
    "#wat, channels, height, width = record['no exercise'][0].shape\n",
    "height, width, channels = record['no exercise'][0].shape\n",
    "video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "for frame in record['no exercise']:\n",
    "    # Ensure the frame data type is uint8\n",
    "    #frame = frame.reshape(3, 224, 224).transpose(1, 2, 0).astype('uint8')\n",
    "    # Write the frame to the video\n",
    "    video_writer.write(frame)\n",
    "\n",
    "video_writer.release()\n",
    "\n",
    "print(\"Video created successfully:\", output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3f8b32ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "224\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "wat, channels, height, width = record['no exercise'][0].shape\n",
    "print(height)\n",
    "print(width)\n",
    "print(channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd2b407",
   "metadata": {},
   "source": [
    "# Create a database of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "dee16be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_pack(\n",
    "    source: str = \"0\",\n",
    "    flip: bool = True,\n",
    "    use_popup: bool = False,\n",
    "    compiled_model_en: CompiledModel = compiled_model_en,\n",
    "    skip_first_frames: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use the \"source\" webcam or video file to run the complete pipeline for action-recognition problem\n",
    "    1. Create a video player to play with target fps\n",
    "    2. Prepare a set of frames to be encoded-decoded\n",
    "    3. Preprocess frame before Encoder\n",
    "    4. Encoder Inference per frame\n",
    "    5. Decoder inference per set of frames\n",
    "    6. Visualize the results\n",
    "\n",
    "    :param: source: webcam \"0\" or video path\n",
    "    :param: flip: to be used by VideoPlayer function for flipping capture image\n",
    "    :param: use_popup: False for showing encoded frames over this notebook, True for creating a popup window.\n",
    "    :param: skip_first_frames: Number of frames to skip at the beginning of the video.\n",
    "    :returns: display video over the notebook or in a popup window\n",
    "\n",
    "    \"\"\"\n",
    "    size = height_en  # Endoder input size - From Cell 5_9\n",
    "    sample_duration = frames2decode  # Decoder input size - From Cell 5_7\n",
    "\n",
    "    # Select frames per second of your source.\n",
    "    fps = 30\n",
    "    player = None\n",
    "\n",
    "    #List of 16-pack vectors\n",
    "    vector_pack = []\n",
    "\n",
    "    #Mediapipe Pose Detection\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    \n",
    "        try:\n",
    "            # Create a video player.\n",
    "            player = utils.VideoPlayer(source, flip=flip, fps=fps, skip_first_frames=skip_first_frames)\n",
    "            # Start capturing.\n",
    "            player.start()\n",
    "            if use_popup:\n",
    "                title = \"Press ESC to Exit\"\n",
    "                cv2.namedWindow(title, cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "            encoder_output = []\n",
    "            counter = 0\n",
    "\n",
    "\n",
    "            while True:\n",
    "                counter = counter + 1\n",
    "\n",
    "                # Read a frame from the video stream.\n",
    "                frame = player.next()\n",
    "                if frame is None:\n",
    "                    print(\"Source ended\")\n",
    "                    break\n",
    "\n",
    "                if counter % 2 == 0:\n",
    "                    # Preprocess frame before Encoder.\n",
    "                    (preprocessed, _) = preprocessing(frame, size)\n",
    "\n",
    "                    # Encoder Inference per frame\n",
    "                    encoder_output.append(encoder(preprocessed, compiled_model_en))\n",
    "                    \n",
    "\n",
    "                    if len(encoder_output) == sample_duration:\n",
    "                        vector_pack.append(encoder_output)\n",
    "                        encoder_output = []\n",
    "\n",
    "        # ctrl-c\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted\")\n",
    "        # Any different error\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            if player is not None:\n",
    "                # Stop capturing.\n",
    "                player.stop()\n",
    "            if use_popup:\n",
    "                cv2.destroyAllWindows()\n",
    "    return vector_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92c5ad3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_vector_pack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m source \u001b[38;5;241m=\u001b[39m cam_id \u001b[38;5;28;01mif\u001b[39;00m USE_WEBCAM \u001b[38;5;28;01melse\u001b[39;00m video_file\n\u001b[0;32m      7\u001b[0m additional_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_first_frames\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflip\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m USE_WEBCAM \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflip\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[1;32m----> 8\u001b[0m vector_pack \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vector_pack\u001b[49m(source\u001b[38;5;241m=\u001b[39msource, use_popup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madditional_options)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_vector_pack' is not defined"
     ]
    }
   ],
   "source": [
    "USE_WEBCAM = False\n",
    "\n",
    "cam_id = 0\n",
    "video_file = \"video_data/pull_up/pull_up_1.mp4\"\n",
    "\n",
    "source = cam_id if USE_WEBCAM else video_file\n",
    "additional_options = {\"skip_first_frames\": 0, \"flip\": False} if not USE_WEBCAM else {\"flip\": True}\n",
    "vector_pack = create_vector_pack(source=source, use_popup=True, **additional_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "fa8c835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_vector(vector_pack):\n",
    "    data_list = []\n",
    "\n",
    "    for pack in vector_pack:\n",
    "        pack_input = np.concatenate(pack, axis=0)\n",
    "        pack_input = pack_input.transpose((2, 0, 1, 3))\n",
    "        pack_input = np.squeeze(pack_input, axis=3)\n",
    "        data_list.append(pack_input[0])\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the videos\n",
    "root_dir = 'video_data'\n",
    "\n",
    "# Initialize dictionary to store lists of big arrays\n",
    "data_dict = {}\n",
    "\n",
    "# Traverse through the directory structure\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    # Check if the current sub-directory contains videos\n",
    "    video_files = [file for file in files if file.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    if video_files:\n",
    "        # Process each video and generate lists of big arrays\n",
    "        for video_file in video_files:\n",
    "            video_path = os.path.join(subdir, video_file)\n",
    "            video_name = os.path.splitext(video_file)[0]  # Extract video name without extension\n",
    "            print(f'Loading video {video_path}...')\n",
    "            vector_pack = create_vector_pack(source=source, use_popup=False, **additional_options)\n",
    "            print('Vector pack created! Concatenating Vectors...')\n",
    "            big_arrays = unpack_vector(vector_pack)\n",
    "            print(f'Succeed. Saving with label {subdir}')\n",
    "            # Store lists of big arrays in dictionary with folder name as key\n",
    "            label = os.path.basename(subdir)\n",
    "            data_dict[(label, video_name)] = big_arrays\n",
    "# Save all lists of big arrays into a single .npz file\n",
    "npz_file_path = \"vecto_database_video.npz\"\n",
    "np.savez(npz_file_path, **{str(key): value for key, value in data_dict.items()})\n",
    "print(f\"Saved big arrays to {npz_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8a825",
   "metadata": {},
   "source": [
    "## Try without video name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b6669a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>big_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>burpee</td>\n",
       "      <td>[[0.181176, 0.08002925, 0.16622949, 0.01320751...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burpee</td>\n",
       "      <td>[[0.3144227, 0.028594825, 0.1010986, 0.0578447...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>burpee</td>\n",
       "      <td>[[0.2852033, 0.26921174, 0.17491572, 0.0464213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>burpee</td>\n",
       "      <td>[[0.17331505, 0.037380286, 0.08150157, 0.02385...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>burpee</td>\n",
       "      <td>[[0.1418101, 0.14380236, 0.030094568, 0.019460...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>sit_up</td>\n",
       "      <td>[[0.17680885, 0.06596319, 0.040142085, 0.05749...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>sit_up</td>\n",
       "      <td>[[0.11307282, 0.26461148, 0.044432614, 0.01717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>sit_up</td>\n",
       "      <td>[[0.22903247, 0.4397676, 0.04068598, 0.0053695...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>sit_up</td>\n",
       "      <td>[[0.20696819, 0.071437016, 0.027337344, 0.0001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>sit_up</td>\n",
       "      <td>[[0.18268344, 0.2107925, 0.009500868, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                          big_array\n",
       "0    burpee  [[0.181176, 0.08002925, 0.16622949, 0.01320751...\n",
       "1    burpee  [[0.3144227, 0.028594825, 0.1010986, 0.0578447...\n",
       "2    burpee  [[0.2852033, 0.26921174, 0.17491572, 0.0464213...\n",
       "3    burpee  [[0.17331505, 0.037380286, 0.08150157, 0.02385...\n",
       "4    burpee  [[0.1418101, 0.14380236, 0.030094568, 0.019460...\n",
       "..      ...                                                ...\n",
       "161  sit_up  [[0.17680885, 0.06596319, 0.040142085, 0.05749...\n",
       "162  sit_up  [[0.11307282, 0.26461148, 0.044432614, 0.01717...\n",
       "163  sit_up  [[0.22903247, 0.4397676, 0.04068598, 0.0053695...\n",
       "164  sit_up  [[0.20696819, 0.071437016, 0.027337344, 0.0001...\n",
       "165  sit_up  [[0.18268344, 0.2107925, 0.009500868, 0.0, 0.0...\n",
       "\n",
       "[166 rows x 2 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from .npz file\n",
    "npz_file_path = \"vecto_database.npz\"\n",
    "loaded_data = np.load(npz_file_path)\n",
    "\n",
    "# Initialize lists to store label and big array data\n",
    "labels = []\n",
    "big_arrays = []\n",
    "\n",
    "# Iterate through loaded data and extract label and big array\n",
    "for label, data_array in loaded_data.items():\n",
    "    labels.extend([label] * len(data_array))\n",
    "    big_arrays.extend(data_array)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'label': labels, 'big_array': big_arrays})\n",
    "\n",
    "# Display DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa35fdc",
   "metadata": {},
   "source": [
    "## Try with video name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "c7026975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>big_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('burpee', 'burpee_1')</td>\n",
       "      <td>[[0.25409243, 0.042802602, 0.16231276, 0.02132...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('burpee', 'burpee_1')</td>\n",
       "      <td>[[0.34191197, 0.008533829, 0.076149285, 0.0106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('burpee', 'burpee_1')</td>\n",
       "      <td>[[0.31739214, 0.12699544, 0.16563262, 0.048489...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('burpee', 'burpee_1')</td>\n",
       "      <td>[[0.20254678, 0.05345901, 0.0992722, 0.0130285...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('burpee', 'burpee_1')</td>\n",
       "      <td>[[0.124611795, 0.20127398, 0.05477627, 0.0, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>('sit_up', 'sit_up_5')</td>\n",
       "      <td>[[0.15490694, 0.12911242, 0.06561741, 0.003459...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>('sit_up', 'sit_up_5')</td>\n",
       "      <td>[[0.16544935, 0.045581326, 0.031468082, 0.0125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>('sit_up', 'sit_up_5')</td>\n",
       "      <td>[[0.28516218, 0.15715006, 0.11490148, 0.029659...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>('sit_up', 'sit_up_5')</td>\n",
       "      <td>[[0.1288017, 0.19007759, 0.087020025, 0.008638...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>('sit_up', 'sit_up_5')</td>\n",
       "      <td>[[0.197517, 0.26498878, 0.026819851, 0.0, 0.04...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label                                          big_array\n",
       "0    ('burpee', 'burpee_1')  [[0.25409243, 0.042802602, 0.16231276, 0.02132...\n",
       "1    ('burpee', 'burpee_1')  [[0.34191197, 0.008533829, 0.076149285, 0.0106...\n",
       "2    ('burpee', 'burpee_1')  [[0.31739214, 0.12699544, 0.16563262, 0.048489...\n",
       "3    ('burpee', 'burpee_1')  [[0.20254678, 0.05345901, 0.0992722, 0.0130285...\n",
       "4    ('burpee', 'burpee_1')  [[0.124611795, 0.20127398, 0.05477627, 0.0, 0....\n",
       "..                      ...                                                ...\n",
       "151  ('sit_up', 'sit_up_5')  [[0.15490694, 0.12911242, 0.06561741, 0.003459...\n",
       "152  ('sit_up', 'sit_up_5')  [[0.16544935, 0.045581326, 0.031468082, 0.0125...\n",
       "153  ('sit_up', 'sit_up_5')  [[0.28516218, 0.15715006, 0.11490148, 0.029659...\n",
       "154  ('sit_up', 'sit_up_5')  [[0.1288017, 0.19007759, 0.087020025, 0.008638...\n",
       "155  ('sit_up', 'sit_up_5')  [[0.197517, 0.26498878, 0.026819851, 0.0, 0.04...\n",
       "\n",
       "[156 rows x 2 columns]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npz_file_path = \"vecto_database_video.npz\"\n",
    "loaded_data = np.load(npz_file_path, allow_pickle=True)\n",
    "\n",
    "# Initialize lists to store folder names, video names, and big arrays\n",
    "folder_names = []\n",
    "#video_names = []\n",
    "big_arrays = []\n",
    "\n",
    "# Iterate through loaded data and extract folder names, video names, and big arrays\n",
    "for label, data in loaded_data.items():\n",
    "    for video_info in data:\n",
    "        folder_names.append(label)\n",
    "        #video_names.append(video_info[0])  # Extracting video name from the first element of video_info\n",
    "        big_arrays.append(video_info)   # Extracting big array from the second element of video_info\n",
    "\n",
    "# Create DataFrame\n",
    "df_v = pd.DataFrame({'label': folder_names, 'big_array': big_arrays})\n",
    "\n",
    "# Display DataFrame\n",
    "df_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae4333",
   "metadata": {},
   "source": [
    "## Test this thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "46097216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source ended\n",
      "Source ended\n"
     ]
    }
   ],
   "source": [
    "test_vector_pack = create_vector_pack(source='video_data/burpee/burpee_1.mp4', use_popup=False, **additional_options)\n",
    "test_vector_pack_1 = create_vector_pack(source='video_data/burpee/burpee_1.mp4', use_popup=False, **additional_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector_unpack = unpack_vector(test_vector_pack)\n",
    "test_vector_unpack_1 = unpack_vector(test_vector_pack_1)\n",
    "print(len(test_vector_unpack),len(test_vector_unpack_1))\n",
    "test_vector_unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector_unpack_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "14e3ddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_pack_vector 1: 0.8925144523382187\n",
      "total_pack_vector 2: 0.9208042696118355\n",
      "total_pack_vector 3: 0.9636142700910568\n",
      "total_pack_vector 4: 0.9319534674286842\n",
      "total_pack_vector 5: 0.9395167827606201\n",
      "total_pack_vector 6: 0.8922161310911179\n",
      "total_pack_vector 7: 0.8287025056779385\n",
      "total_pack_vector 8: 0.8615151457488537\n",
      "total_pack_vector 9: 0.805098831653595\n",
      "total_pack_vector 10: 0.8617491126060486\n",
      "total_pack_vector 11: 0.8282929845154285\n",
      "total_pack_vector 12: 0.7571482695639133\n",
      "total_pack_vector 13: 0.7663170322775841\n",
      "total_pack_vector 14: 0.7874786108732224\n",
      "total_pack_vector 15: 0.7756083495914936\n",
      "total_pack_vector 16: 0.7936922423541546\n",
      "total_pack_vector 17: 0.8218375742435455\n",
      "total_pack_vector 18: 0.7935625314712524\n",
      "total_pack_vector 19: 0.76043919660151\n",
      "total_pack_vector 20: 0.7768196668475866\n",
      "total_pack_vector 21: 0.7550389133393764\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[404], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_vector_unpack[\u001b[38;5;241m0\u001b[39m])):\n\u001b[1;32m----> 4\u001b[0m     value \u001b[38;5;241m=\u001b[39m cosine_similarity(\u001b[43mtest_vector_unpack_1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[j],test_vector_unpack[i][j])\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#print(f'pack_vector {i+1}: {value}')\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m.\u001b[39mappend(value)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_vector_unpack)):\n",
    "    list = []\n",
    "    for j in range(len(test_vector_unpack[0])):\n",
    "        value = cosine_similarity(test_vector_unpack_1[i][j],test_vector_unpack[i][j])\n",
    "        #print(f'pack_vector {i+1}: {value}')\n",
    "        list.append(value)\n",
    "    total = sum(list)/16\n",
    "    #print(list)\n",
    "    print(f'total_pack_vector {i+1}: {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "14f2ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Euclidean distance between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Function to compute lowest Euclidean distance for each row\n",
    "def compute_greatest_similarity(row):\n",
    "    dataframe_big_array = row['big_array']\n",
    "    num_vectors = len(dataframe_big_array)\n",
    "    greatest_similarity = -np.inf  # Initialize with a very small value\n",
    "    for i in range(num_vectors):\n",
    "        similarity_sum = 0\n",
    "        for j in range(num_vectors):\n",
    "            similarity = cosine_similarity(dataframe_big_array[j], test_vector_unpack[6][(i + j) % num_vectors])\n",
    "            similarity_sum += (similarity)\n",
    "        greatest_similarity = max(greatest_similarity, similarity_sum / num_vectors)\n",
    "    return greatest_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "e4c60623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>lowest_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>('push_up', 'push_up_5')</td>\n",
       "      <td>0.711297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>('push_up', 'push_up_3')</td>\n",
       "      <td>0.710972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('burpee', 'burpee_2')</td>\n",
       "      <td>0.708390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('sit_up', 'sit_up_1')</td>\n",
       "      <td>0.707530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('push_up', 'push_up_1')</td>\n",
       "      <td>0.707335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>('sit_up', 'sit_up_2')</td>\n",
       "      <td>0.707318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>('push_up', 'push_up_2')</td>\n",
       "      <td>0.707069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>('sit_up', 'sit_up_3')</td>\n",
       "      <td>0.706944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('pull_up', 'pull_up_2')</td>\n",
       "      <td>0.706341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>('sit_up', 'sit_up_5')</td>\n",
       "      <td>0.706209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('pull_up', 'pull_up_1')</td>\n",
       "      <td>0.705886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('burpee', 'burpee_1')</td>\n",
       "      <td>0.705770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('push_up', 'push_up_4')</td>\n",
       "      <td>0.705664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>('sit_up', 'sit_up_4')</td>\n",
       "      <td>0.695912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       label  lowest_distance\n",
       "8   ('push_up', 'push_up_5')         0.711297\n",
       "6   ('push_up', 'push_up_3')         0.710972\n",
       "1     ('burpee', 'burpee_2')         0.708390\n",
       "9     ('sit_up', 'sit_up_1')         0.707530\n",
       "4   ('push_up', 'push_up_1')         0.707335\n",
       "10    ('sit_up', 'sit_up_2')         0.707318\n",
       "5   ('push_up', 'push_up_2')         0.707069\n",
       "11    ('sit_up', 'sit_up_3')         0.706944\n",
       "3   ('pull_up', 'pull_up_2')         0.706341\n",
       "13    ('sit_up', 'sit_up_5')         0.706209\n",
       "2   ('pull_up', 'pull_up_1')         0.705886\n",
       "0     ('burpee', 'burpee_1')         0.705770\n",
       "7   ('push_up', 'push_up_4')         0.705664\n",
       "12    ('sit_up', 'sit_up_4')         0.695912"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to each row in the DataFrame\n",
    "results_df = df_v.copy()\n",
    "results_df['lowest_distance'] = results_df.apply(compute_greatest_similarity, axis=1)\n",
    "#sorted_results = results_df.sort_values(by='lowest_distance', ascending=False)\n",
    "#sorted_results\n",
    "averages_df = results_df.groupby('label')['lowest_distance'].mean().reset_index()\n",
    "averages_df_sorted = averages_df.sort_values(by='lowest_distance', ascending=False)\n",
    "averages_df_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
